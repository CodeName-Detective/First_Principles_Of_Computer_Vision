{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notes:\n",
    "1. All of your implementation for task 1 should be in this file. \n",
    "2. Please Read the instructions and do not modify the input and output formats of function detect_faces().\n",
    "3. If you want to show an image for debugging, please use show_image() function in helper.py.\n",
    "4. Please do NOT save any intermediate files in your final submission.\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "\n",
    "from typing import Dict, List\n",
    "from utils import show_image\n",
    "\n",
    "\n",
    "'''\n",
    "Please do NOT add any imports. The allowed libraries are already imported for you.\n",
    "'''\n",
    "\n",
    "def detect_faces(img: np.ndarray) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img : input image is an np.ndarray represent an input image of shape H x W x 3.\n",
    "            H is the height of the image, W is the width of the image. 3 is the [R, G, B] channel (NOT [B, G, R]!).\n",
    "\n",
    "    Returns:\n",
    "        detection_results: a python nested list. \n",
    "            Each element is the detected bounding boxes of the faces (may be more than one faces in one image).\n",
    "            The format of detected bounding boxes a python list of float with length of 4. It should be formed as \n",
    "            [topleft-x, topleft-y, box-width, box-height] in pixels.\n",
    "    \"\"\"\n",
    "    detection_results: List[List[float]] = [] # Please make sure your output follows this data format.\n",
    "\n",
    "    # Add your code here. Do not modify the return and input arguments.\n",
    "\n",
    "    # Load the Haar Cascade Classifier for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Convert image to grayscale as Haar Cascade works on grayscale images\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    detection_results = [[float(x), float(y), float(w), float(h)] for (x, y, w, h) in faces]\n",
    "    \n",
    "    return detection_results\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"cse 573 homework 4.\")\n",
    "    parser.add_argument(\n",
    "        \"--input_path\", type=str, default=\"data/validation_folder/images\",\n",
    "        help=\"path to validation or test folder\")\n",
    "    parser.add_argument(\n",
    "        \"--output\", type=str, default=\"./result_task1.json\",\n",
    "        help=\"path to the characters folder\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def save_results(result_dict, filename):\n",
    "    results = []\n",
    "    results = result_dict\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "\n",
    "def check_output_format(faces, img, img_name):\n",
    "    if not isinstance(faces, list):\n",
    "        print('Wrong output type for image %s! Should be a %s, but you get %s.' % (img_name, list, type(faces)))\n",
    "        return False\n",
    "    for i, face in enumerate(faces):\n",
    "        if not isinstance(face, list):\n",
    "            print('Wrong bounding box type in image %s the %dth face! Should be a %s, but you get %s.' % (img_name, i, list, type(face)))\n",
    "            return False\n",
    "        if not len(face) == 4:\n",
    "            print('Wrong bounding box format in image %s the %dth face! The length should be %s , but you get %s.' % (img_name, i, 4, len(face)))\n",
    "            return False\n",
    "        for j, num in enumerate(face):\n",
    "            if not isinstance(num, float):\n",
    "                print('Wrong bounding box type in image %s the %dth face! Should be a list of %s, but you get a list of %s.' % (img_name, i, float, type(num)))\n",
    "                return False\n",
    "        if face[0] >= img.shape[1] or face[1] >= img.shape[0] or face[0] + face[2] >= img.shape[1] or face[1] + face[3] >= img.shape[0]:\n",
    "            print('Warning: Wrong bounding box in image %s the %dth face exceeds the image size!' % (img_name, i))\n",
    "            print('One possible reason of this is incorrect bounding box format. The format should be [topleft-x, topleft-y, box-width, box-height] in pixels.')\n",
    "    return True\n",
    "\n",
    "\n",
    "def batch_detection(img_dir):\n",
    "    res = {}\n",
    "    for img_name in sorted(os.listdir(img_dir)):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        faces = detect_faces(img)\n",
    "        if not check_output_format(faces, img, img_name):\n",
    "            print('Wrong output format!')\n",
    "            sys.exit(2)\n",
    "        res[img_name] = faces\n",
    "    return res\n",
    "\n",
    "# def main():\n",
    "\n",
    "#     args = parse_args()\n",
    "#     path, filename = os.path.split(args.output)\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "#     result_list = batch_detection(args.input_path)\n",
    "#     save_results(result_list, args.output)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = batch_detection('data/validation_folder/images/')\n",
    "save_results(result_list, 'result_task1_val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/validation_folder/images/img_1.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "faces = detect_faces(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9281\n"
     ]
    }
   ],
   "source": [
    "! python3 ComputeFBeta/ComputeFBeta.py --preds result_task1_val.json --groundtruth data/validation_folder/ground-truth.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min([1, 1.25*0.9281])*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 task1.py --input_path data/validation_folder/images --output ./result_task1_val.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9281\n"
     ]
    }
   ],
   "source": [
    "! python3 ComputeFBeta/ComputeFBeta.py --preds result_task1_val.json --groundtruth data/validation_folder/ground-truth.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min([1, 1.25*0.9281])*40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images(img1, img2, descriptor1, descriptor2, min_match_threshold=10):\n",
    "    matches = match_features(descriptor1, descriptor2, min_match_threshold=10)\n",
    "\n",
    "    if len(matches) > min_match_threshold:\n",
    "        # Get the matching keypoints for each of the images\n",
    "        img1_pts = np.float32([descriptor1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        img2_pts = np.float32([descriptor2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Compute the homography matrix\n",
    "        M, _ = cv2.findHomography(img1_pts, img2_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        # Get the dimensions of the two images\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "\n",
    "        # Warp the images based on the homography matrix\n",
    "        warp_img1 = cv2.warpPerspective(img1, M, (w1 + w2, max(h1, h2)))\n",
    "        warp_img1[0:h2, 0:w2] = img2\n",
    "\n",
    "        return warp_img1\n",
    "    else:\n",
    "        print(\"Not enough matches found - {}/{}\".format(len(matches), min_match_threshold))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Only add your code inside the function (including newly improted packages). \n",
    "#  You can design a new function and call the new function in the given functions. \n",
    "# 3. Not following the project guidelines will result in a 10% reduction in grades\n",
    "# 4 . If you want to show an image for debugging, please use show_image() function in helper.py.\n",
    "# 5. Please do NOT save any intermediate files in your final submission.\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import json\n",
    "import array as arr\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"cse 573 homework 4.\")\n",
    "    parser.add_argument(\n",
    "        \"--input_path\", type=str, default=\"data/images_panaroma\",\n",
    "        help=\"path to images for panaroma construction\")\n",
    "    parser.add_argument(\n",
    "        \"--output_overlap\", type=str, default=\"./task2_overlap.txt\",\n",
    "        help=\"path to the overlap result\")\n",
    "    parser.add_argument(\n",
    "        \"--output_panaroma\", type=str, default=\"./task2_result.png\",\n",
    "        help=\"path to final panaroma image \")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def extract_features(image):\n",
    "    # Initialize SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "    if image.dtype != np.uint8:\n",
    "        image = cv2.convertScaleAbs(image)\n",
    "    # Detect keypoints and compute descriptors\n",
    "    key_pointers , descriptors = sift.detectAndCompute(image, None)\n",
    "    return key_pointers, descriptors\n",
    "\n",
    "def match_features(descriptors1, descriptors2, min_distance_threshold):\n",
    "    \n",
    "    d1 = descriptors1[:, np.newaxis, :]\n",
    "    d2 = descriptors2[np.newaxis, :, :]\n",
    "\n",
    "    \n",
    "    distances = np.linalg.norm(d1 - d2, axis=2)\n",
    "\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "    closest_indices = np.argmin(distances, axis=1)\n",
    "\n",
    "    valid_matches = min_distances <= min_distance_threshold\n",
    "\n",
    "    matches = [[i,closest_indices[i]] for i in range(len(descriptors1)) if valid_matches[i]]\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "def find_image_pairs(overlap_matrix):\n",
    "    pairs = []\n",
    "    n = len(overlap_matrix)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if overlap_matrix[i][j] == 1:\n",
    "                # Add the pair (i, j) if the images overlap\n",
    "                pairs.append((i, j))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "\n",
    "def get_translation_matrix(img, H):\n",
    "    \n",
    "    # Define corners in homogeneous coordinates\n",
    "    height, width = img.shape[:2]\n",
    "    corners = np.array([\n",
    "        [0, 0, 1],                               # Top-left\n",
    "        [0, height - 1, 1],                       # Bottom-left\n",
    "        [width - 1, 0, 1],                        # Top-right\n",
    "        [width - 1, height - 1, 1]                # Bottom-right\n",
    "    ])\n",
    "\n",
    "    # Transform corners using the homography matrix\n",
    "    transformed_corners = np.matmul(H,corners.T).T\n",
    "\n",
    "    # Normalize to convert from homogeneous coordinates\n",
    "    transformed_corners = transformed_corners / transformed_corners[:, 2][:, np.newaxis]\n",
    "\n",
    "    # Calculate translation values\n",
    "    translation_x = int(max(-transformed_corners[:, 0].min(), 0))\n",
    "    translation_y = int(max(-transformed_corners[:, 1].min(), 0))\n",
    "\n",
    "    # Calculate dimensions of the transformed image\n",
    "    new_H = int(max(transformed_corners[:, 1].max(), 0)) + translation_y\n",
    "    new_W = int(max(transformed_corners[:, 0].max(), 0)) + translation_x\n",
    "\n",
    "    # Create translation matrix\n",
    "    M = np.array([[1, 0, translation_x], [0, 1, translation_y], [0, 0, 1]], dtype=float)\n",
    "\n",
    "    return M, new_H, new_W\n",
    "\n",
    "\n",
    "def stitch_images(img1, img2, key_pointers1, key_pointers2, matches):\n",
    "    \n",
    "    img1_pts = []\n",
    "    img2_pts = []\n",
    "    \n",
    "    for m in matches:\n",
    "        img1_pts.append(key_pointers1[m[0]].pt)\n",
    "        img2_pts.append(key_pointers2[m[1]].pt)\n",
    "        \n",
    "    img1_pts = np.array(img1_pts)\n",
    "    img2_pts = np.array(img2_pts)\n",
    "    \n",
    "    # Compute the homography matrix\n",
    "    H, _ = cv2.findHomography(img2_pts, img1_pts, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    \n",
    "    translation_matrix, new_H, new_W = get_translation_matrix(img1, H)\n",
    "    \n",
    "    H_img2 = np.matmul(translation_matrix, H)\n",
    "    H_img1 = np.matmul(translation_matrix,np.identity(3))\n",
    "    \n",
    "    warpim2 = cv2.warpPerspective(img2, H_img2, (new_W, new_H))\n",
    "    warpim1 = cv2.warpPerspective(img1, H_img1, (new_W, new_H))\n",
    "  \n",
    "    blended_img = np.where(warpim1>0, warpim1, warpim2)\n",
    "    return blended_img\n",
    "\n",
    "\n",
    "def stitch(inp_path, imgmark, N=4, savepath=''): \n",
    "    \"The output image should be saved in the savepath.\"\n",
    "    \"The intermediate overlap relation should be returned as NxN a one-hot(only contains 0 or 1) array.\"\n",
    "    \"Do NOT modify the code provided.\"\n",
    "    imgpath = [f'{inp_path}/{imgmark}_{n}.png' for n in range(1,N+1)]\n",
    "    imgs = []\n",
    "    for ipath in imgpath:\n",
    "        img = cv2.imread(ipath)\n",
    "        imgs.append(img)\n",
    "    \"Start you code here\"\n",
    "    overlap_arr = np.eye(len(imgs), dtype=int)\n",
    "    descriptors = {}\n",
    "    key_pointers = {}\n",
    "    matched_points_set = {}\n",
    "    stitched_image = None\n",
    "    processed_set = set()\n",
    "    for i, img1 in enumerate(imgs):\n",
    "        for j, img2 in enumerate(imgs):\n",
    "            if i != j:\n",
    "                # Extract features and match\n",
    "                if i not in descriptors.keys():\n",
    "                    keys, descs = extract_features(img1)\n",
    "                    key_pointers[i] = keys\n",
    "                    descriptors[i] = descs\n",
    "                if j not in descriptors.keys():\n",
    "                    keys,descs= extract_features(img2)\n",
    "                    key_pointers[j] = keys\n",
    "                    descriptors[j] = descs\n",
    "                    \n",
    "                matches = match_features(descriptors[i], descriptors[j], 50)\n",
    "                \n",
    "                matched_points_set[(i,j)] = matches\n",
    "                \n",
    "                min_match_threshold = 10 # Value to tune\n",
    "                if len(matches) > min_match_threshold:\n",
    "                    overlap_arr[i, j] = 1\n",
    "                    if stitched_image is None:\n",
    "                        stitched_image = stitch_images(imgs[i], imgs[j], key_pointers[i], key_pointers[j], matched_points_set[(i,j)])\n",
    "                        processed_set.add(i)\n",
    "                        processed_set.add(j)\n",
    "                    elif stitched_image is not None and j not in processed_set:\n",
    "                        key_pointers_stitiched_image, descriptors_stitiched_image = extract_features(stitched_image)\n",
    "                        matched_points = match_features(descriptors_stitiched_image, descriptors[j], 50)\n",
    "                        stitched_image = stitch_images(stitched_image, imgs[j], key_pointers_stitiched_image, key_pointers[j], matched_points)\n",
    "                        processed_set.add(j)\n",
    "                else:\n",
    "                    overlap_arr[i, j] = 0\n",
    "                \n",
    "    with open('overlap_arr.json', 'w') as f:\n",
    "        json.dump(overlap_arr.tolist(), f)\n",
    "    \n",
    "    if stitched_image is not None:\n",
    "        cv2.imwrite(savepath, stitched_image)\n",
    "                \n",
    "    return overlap_arr\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     #task2\n",
    "#     args = parse_args()\n",
    "#     overlap_arr = stitch(args.input_path, 't2', N=4, savepath=f'{args.output_panaroma}')\n",
    "#     with open(f'{args.output_overlap}', 'w') as outfile:\n",
    "#         json.dump(overlap_arr.tolist(), outfile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [0, 1, 1, 0],\n",
       "       [1, 1, 0, 1]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stitch('data/images_panaroma/', 't2', N=4, savepath='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('data/images_panaroma/t2_1.png')\n",
    "img4 = cv2.imread('data/images_panaroma/t2_4.png')\n",
    "\n",
    "descriptors1 = extract_features(img1)\n",
    "descriptors2 = extract_features(img4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_homogeneous_coordinate_corners(img):\n",
    "    height, width = img.shape[:2]\n",
    "    corners = [\n",
    "        np.array([0, 0, 1]),                 # Top-left corner\n",
    "        np.array([0, height - 1, 1]),        # Bottom-left corner\n",
    "        np.array([width - 1, 0, 1]),         # Top-right corner\n",
    "        np.array([width - 1, height - 1, 1]) # Bottom-right corner\n",
    "    ]\n",
    "    return np.array(corners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   1],\n",
       "       [  0, 370,   1],\n",
       "       [542,   0,   1],\n",
       "       [542, 370,   1]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_homogeneous_coordinate_corners(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471.8114"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(descriptors1[0]-descriptors2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< cv2.DMatch 0x131f398f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  6.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   4.]]\n",
      "\n",
      " [[ 29.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.  34.]]\n",
      "\n",
      " [[ 81.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0. 134.]]\n",
      "\n",
      " [[ 78.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0. 109.]]\n",
      "\n",
      " [[  6.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.  13.]]\n",
      "\n",
      " [[112.   2.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.  35.]]\n",
      "\n",
      " [[134.  23.]]\n",
      "\n",
      " [[  2.   1.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.  95.]]\n",
      "\n",
      " [[ 48.  12.]]\n",
      "\n",
      " [[ 37.  35.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.  33.]]\n",
      "\n",
      " [[  6.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.  18.]]\n",
      "\n",
      " [[ 93.  11.]]\n",
      "\n",
      " [[ 11.   2.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.  23.]]\n",
      "\n",
      " [[134.  81.]]\n",
      "\n",
      " [[132.  51.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   4.]]\n",
      "\n",
      " [[ 11.  16.]]\n",
      "\n",
      " [[134. 134.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  2.   1.]]\n",
      "\n",
      " [[ 18.  25.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  1.  10.]]\n",
      "\n",
      " [[ 10.   8.]]\n",
      "\n",
      " [[ 97.  89.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   4.]]\n",
      "\n",
      " [[  4.  12.]]\n",
      "\n",
      " [[134. 134.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[  0.   0.]]\n",
      "\n",
      " [[101. 121.]]\n",
      "\n",
      " [[  1.   0.]]\n",
      "\n",
      " [[  0.   0.]]]\n",
      "[  6.   0.   0.   0.   0.   0.   0.   4.  31.   0.   0.   0.   0.   0.\n",
      "   0.  40.  82.   0.   0.   0.   0.   0.   0. 131.  77.   0.   0.   0.\n",
      "   0.   0.   0. 106.   7.   0.   0.   0.   0.   0.   0.  13. 121.   1.\n",
      "   0.   0.   0.   0.   0.  39. 131.  20.   4.   1.   0.   0.   0.  97.\n",
      "  45.  12.  45.  38.   0.   0.   0.  34.   7.   0.   0.   0.   0.   0.\n",
      "   0.  18.  99.  17.  15.   2.   0.   0.   0.  18. 131.  86. 131.  46.\n",
      "   0.   0.   0.   2.   7.  14. 131. 131.   0.   0.   0.   0.   2.   1.\n",
      "  22.  28.   0.   0.   0.   7.   8.  10. 105.  82.   0.   0.   0.   2.\n",
      "   2.  12. 131. 129.   0.   0.   0.   0.   0.   0. 103. 122.   0.   0.\n",
      "   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "for m in matches:\n",
    "    print(descriptors1[m.queryIdx].reshape(-1, 1, 2))\n",
    "    print(descriptors2[m.trainIdx])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471.8114"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.sum(np.square(descriptors1[0] - descriptors2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageStitching_noClip(im1, im2, H2to1):\n",
    "    '''\n",
    "    Returns a panorama of im1 and im2 using the given \n",
    "    homography matrix without cliping.\n",
    "    \n",
    "    INPUTS\n",
    "        im1 and im2 - images to be stitched.\n",
    "        H2to1- the homography matrix.\n",
    "    OUTPUT\n",
    "        img_pano - the panorama image.\n",
    "    ''' \n",
    "    # YOUR CODE HERE\n",
    "    def createmask(image):\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]))\n",
    "        mask[1:image.shape[0]-1,1:image.shape[1]-1] = 1\n",
    "        mask = distance_transform_edt(mask)\n",
    "        mask = mask / np.max(mask)\n",
    "        return mask\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    left_top = np.array([0, 0, 1])\n",
    "    left_bottom = np.array([0, im2.shape[0] - 1, 1])\n",
    "    right_top = np.array([im2.shape[1] - 1, 0, 1])\n",
    "    right_bottom = np.array([im2.shape[1] - 1, im2.shape[0] - 1, 1])\n",
    "    \n",
    "    left_top_est = H2to1 @ left_top\n",
    "    left_bottom_est = H2to1 @ left_bottom\n",
    "    right_top_est = H2to1 @ right_top\n",
    "    right_bottom_est = H2to1 @ right_bottom\n",
    "    \n",
    "    left_top_est = left_top_est/left_top_est[2]\n",
    "    left_bottom_est = left_bottom_est/left_bottom_est[2]\n",
    "    right_top_est = right_top_est/right_top_est[2]\n",
    "    right_bottom_est = right_bottom_est/right_bottom_est[2]\n",
    "    \n",
    "    transx = int(max(-left_top_est[0], -left_bottom_est[0], 0))\n",
    "    transy = int(max(-left_top_est[1], -right_top_est[1], 0))\n",
    "    \n",
    "    imH = max(left_bottom_est[1].astype(int) , right_bottom_est[1].astype(int)) + transy\n",
    "    imW = max(right_top_est[0].astype(int) , right_bottom_est[0].astype(int)) + transx\n",
    "    \n",
    "    M = np.array([[1, 0, transx], \n",
    "                  [0 , 1, transy], \n",
    "                  [0, 0, 1]]).astype(float)\n",
    "    print(M)\n",
    "    \n",
    "    warpim2 = cv2.warpPerspective(im2, M@H2to1, (imW, imH))\n",
    "    warpim1 = cv2.warpPerspective(im1, M@np.identity(3), (imW, imH))\n",
    "    warpim2 = warpim2/255\n",
    "    warpim1 = warpim1/255\n",
    "    \n",
    "    maskim1 = createmask(im1)\n",
    "    maskim2 = createmask(im2)\n",
    "    \n",
    "    warpmask1 = cv2.warpPerspective(maskim1, M@np.identity(3), (imW, imH))\n",
    "    warpmask2 = cv2.warpPerspective(maskim2, M@H2to1, (imW, imH))\n",
    "    summask = warpmask1 + warpmask2\n",
    "    \n",
    "    warpmask1 = np.divide(warpmask1, summask, out=np.zeros_like(warpmask1), where=summask!=0)\n",
    "    warpmask2 = np.divide(warpmask2, summask, out=np.zeros_like(warpmask2), where=summask!=0)\n",
    "    \n",
    "    warpmask1 = np.expand_dims(warpmask1, axis = 2)\n",
    "    warpmask1 = np.tile(warpmask1, (1,1,3))\n",
    "    \n",
    "    warpmask2 = np.expand_dims(warpmask2, axis = 2)\n",
    "    warpmask2 = np.tile(warpmask2, (1,1,3))\n",
    "    # img = cv2.addWeighted(mountain, 0.3, dog, 0.7, 0)\n",
    "    img_pano = warpim1*warpmask1 + warpim2*warpmask2\n",
    "\n",
    "#     plt.figure(figsize = (18., 18.))\n",
    "#     plt.imshow(img_pano)\n",
    "    \n",
    "    return img_pano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "b = {}\n",
    "\n",
    "def fun():\n",
    "    return a, b\n",
    "\n",
    "a[1], b[2] = fun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {...}}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {...}}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 task2.py --input_path data/images_panaroma --output_overlap ./task2_overlap.txt --output_panaroma ./task2_result.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
